!<arch>
//                                              24        `
batchNormalization.cc/

utils.cc/       0           0     0     644     507       `
#include "utils.h"

namespace keras2cpp {
    Stream::Stream(const std::string& filename)
    : stream_(filename, std::ios::binary) {
        stream_.exceptions();
        if (!stream_.is_open())
            throw std::runtime_error("Cannot open " + filename);
    }

    Stream& Stream::reads(char* ptr, size_t count) {
        stream_.read(ptr, static_cast<ptrdiff_t>(count));
        if (!stream_)
            throw std::runtime_error("File read failure");
        return *this;
    }
}

tensor.cc/      0           0     0     644     3498      `
﻿#include "tensor.h"

namespace keras2cpp {
    Tensor::Tensor(Stream& file, size_t rank) : Tensor() {
        kassert(rank);

        dims_.reserve(rank);
        std::generate_n(std::back_inserter(dims_), rank, [&file] {
            unsigned stride = file;
            kassert(stride > 0);
            return stride;
        });

        data_.resize(size());
        file.reads(reinterpret_cast<char*>(data_.data()), sizeof(float) * size());
    }

    Tensor Tensor::unpack(size_t row) const noexcept {
        kassert(ndim() >= 2);
        size_t pack_size = std::accumulate(dims_.begin() + 1, dims_.end(), 0u);

        auto base = row * pack_size;
        auto first = begin() + cast(base);
        auto last = begin() + cast(base + pack_size);

        Tensor x;
        x.dims_ = std::vector<size_t>(dims_.begin() + 1, dims_.end());
        x.data_ = std::vector<float>(first, last);
        return x;
    }

    Tensor Tensor::select(size_t row) const noexcept {
        auto x = unpack(row);
        x.dims_.insert(x.dims_.begin(), 1);
        return x;
    }

    Tensor& Tensor::operator+=(const Tensor& other) noexcept {
        kassert(dims_ == other.dims_);
        std::transform(begin(), end(), other.begin(), begin(), std::plus<>());
        return *this;
    }

    Tensor& Tensor::operator*=(const Tensor& other) noexcept {
        kassert(dims_ == other.dims_);
        std::transform(begin(), end(), other.begin(), begin(), std::multiplies<>());
        return *this;
    }

    Tensor Tensor::fma(const Tensor& scale, const Tensor& bias) const noexcept {
        kassert(dims_ == scale.dims_);
        kassert(dims_ == bias.dims_);

        Tensor result;
        result.dims_ = dims_;
        result.data_.resize(data_.size());

        auto k_ = scale.begin();
        auto b_ = bias.begin();
        auto r_ = result.begin();
        for (auto x_ = begin(); x_ != end();)
            *(r_++) = *(x_++) * *(k_++) + *(b_++);

        return result;
    }

    Tensor Tensor::dot(const Tensor& other) const noexcept {
        kassert(ndim() == 2);
        kassert(other.ndim() == 2);
        kassert(dims_[1] == other.dims_[1]);

        Tensor tmp {dims_[0], other.dims_[0]};

        auto ts = cast(tmp.dims_[1]);
        auto is = cast(dims_[1]);

        auto i_ = begin();
        for (auto t0 = tmp.begin(); t0 != tmp.end(); t0 += ts, i_ += is) {
            auto o_ = other.begin();
            for (auto t1 = t0; t1 != t0 + ts; ++t1, o_ += is)
                *t1 = std::inner_product(i_, i_ + is, o_, 0.f);
        }
        return tmp;
    }

    void Tensor::print() const noexcept {
        std::vector<size_t> steps(ndim());
        std::partial_sum(
            dims_.rbegin(), dims_.rend(), steps.rbegin(), std::multiplies<>());

        size_t count = 0;
        for (auto&& it : data_) {
            for (auto step : steps)
                if (count % step == 0)
                    printf("[");
            printf("%f", static_cast<double>(it));
            ++count;
            for (auto step : steps)
                if (count % step == 0)
                    printf("]");
            if (count != steps[0])
                printf(", ");
        }
        printf("\n");
    }

    void Tensor::print_shape() const noexcept {
        printf("(");
        size_t count = 0;
        for (auto&& dim : dims_) {
            printf("%zu", dim);
            if ((++count) != dims_.size())
                printf(", ");
        }
        printf(")\n");
    }
}model.cc/       0           0     0     644     1919      `
﻿#include "model.h"
#include "layers/conv1d.h"
#include "layers/conv2d.h"
#include "layers/dense.h"
#include "layers/elu.h"
#include "layers/embedding.h"
#include "layers/flatten.h"
#include "layers/locally1d.h"
#include "layers/locally2d.h"
#include "layers/lstm.h"
#include "layers/maxPooling2d.h"
#include "layers/batchNormalization.h"

namespace keras2cpp {
    std::unique_ptr<BaseLayer> Model::make_layer(Stream& file) {
        switch (static_cast<unsigned>(file)) {
            case Dense:
                return layers::Dense::make(file);
            case Conv1D:
                return layers::Conv1D::make(file);
            case Conv2D:
                return layers::Conv2D::make(file);
            case LocallyConnected1D:
                return layers::LocallyConnected1D::make(file);
            case LocallyConnected2D:
                return layers::LocallyConnected2D::make(file);
            case Flatten:
                return layers::Flatten::make(file);
            case ELU:
                return layers::ELU::make(file);
            case Activation:
                return layers::Activation::make(file);
            case MaxPooling2D:
                return layers::MaxPooling2D::make(file);
            case LSTM:
                return layers::LSTM::make(file);
            case Embedding:
                return layers::Embedding::make(file);
            case BatchNormalization:
                return layers::BatchNormalization::make(file);
        }
        return nullptr;
    }

    Model::Model(Stream& file) {
        auto count = static_cast<unsigned>(file);
        layers_.reserve(count);
        for (size_t i = 0; i != count; ++i)
            layers_.push_back(make_layer(file));
    }

    Tensor Model::operator()(const Tensor& in) const noexcept {
        Tensor out = in;
        for (auto&& layer : layers_)
            out = (*layer)(out);
        return out;
    }
}

baseLayer.cc/   0           0     0     644     90        `
#include "baseLayer.h"
namespace keras2cpp {
    BaseLayer::~BaseLayer() = default;
}
activation.cc/  0           0     0     644     3332      `
﻿#include "activation.h"
namespace keras2cpp{
    namespace layers{
        Activation::Activation(Stream& file) : type_(file) {
            switch (type_) {
            case Linear:
            case Relu:
            case Elu:
            case SoftPlus:
            case SoftSign:
            case HardSigmoid:
            case Sigmoid:
            case Tanh:
            case SoftMax:
                return;
            }
            kassert(false);
        }

        Tensor Activation::operator()(const Tensor& in) const noexcept {
            Tensor out {in.size()};
            out.dims_ = in.dims_;

            switch (type_) {
            case Linear:
                std::copy(in.begin(), in.end(), out.begin());
                break;
            case Relu:
                std::transform(in.begin(), in.end(), out.begin(), [](float x) {
                    if (x < 0.f)
                        return 0.f;
                    return x;
                });
                break;
            case Elu:
                std::transform(in.begin(), in.end(), out.begin(), [](float x) {
                    if (x < 0.f)
                        return std::expm1(x);
                    return x;
                });
                break;
            case SoftPlus:
                std::transform(in.begin(), in.end(), out.begin(), [](float x) {
                    return std::log1p(std::exp(x));
                });
                break;
            case SoftSign:
                std::transform(in.begin(), in.end(), out.begin(), [](float x) {
                    return x / (1.f + std::abs(x));
                });
                break;
            case HardSigmoid:
                std::transform(in.begin(), in.end(), out.begin(), [](float x) {
                    if (x <= -2.5f)
                        return 0.f;
                    if (x >= 2.5f)
                        return 1.f;
                    return (x * .2f) + .5f;
                });
                break;
            case Sigmoid:
                std::transform(in.begin(), in.end(), out.begin(), [](float x) {
                    float z = std::exp(-std::abs(x));
                    if (x < 0)
                        return z / (1.f + z);
                    return 1.f / (1.f + z);
                });
                break;
            case Tanh:
                std::transform(in.begin(), in.end(), out.begin(), [](float x) {
                    return std::tanh(x);
                });
                break;
            case SoftMax: {
                auto channels = cast(in.dims_.back());
                kassert(channels > 1);

                Tensor tmp = in;
                std::transform(in.begin(), in.end(), tmp.begin(), [](float x) {
                    return std::exp(x);
                });

                auto out_ = out.begin();
                for (auto t_ = tmp.begin(); t_ != tmp.end(); t_ += channels) {
                    // why std::reduce not in libstdc++ yet?
                    auto norm = 1.f / std::accumulate(t_, t_ + channels, 0.f);
                    std::transform(
                        t_, t_ + channels, out_, [norm](float x) { return norm * x; });
                    out_ += channels;
                }
                break;
            }
            }
            return out;
        }
    }
}flatten.cc/     0           0     0     644     195       `
﻿#include "flatten.h"
namespace keras2cpp{
    namespace layers{
        Tensor Flatten::operator()(const Tensor& in) const noexcept {
            return Tensor(in).flatten();
        }
    }
}
/0              0           0     0     644     358       `
﻿#include "batchNormalization.h"
namespace keras2cpp{
    namespace layers{
        BatchNormalization::BatchNormalization(Stream& file)
        : weights_(file), biases_(file) {}
        Tensor BatchNormalization::operator()(const Tensor& in) const noexcept {
            kassert(in.ndim());
            return in.fma(weights_, biases_);
        }
    }
}conv1d.cc/      0           0     0     644     1056      `
﻿#include "conv1d.h"
namespace keras2cpp{
    namespace layers{
        Conv1D::Conv1D(Stream& file)
        : weights_(file, 3), biases_(file), activation_(file) {}

        Tensor Conv1D::operator()(const Tensor& in) const noexcept {
            kassert(in.dims_[1] == weights_.dims_[2]);

            auto& ww = weights_.dims_;

            size_t offset = ww[1] - 1;
            auto tmp = Tensor::empty(in.dims_[0] - offset, ww[0]);

            auto ws0 = cast(ww[2] * ww[1]);
            auto ws1 = cast(ww[2]);

            auto tx = cast(tmp.dims_[0]);

            auto i_ptr = in.begin();
            auto b_ptr = biases_.begin();
            auto t_ptr = std::back_inserter(tmp.data_);

            for (ptrdiff_t x = 0; x < tx; ++x) {
                auto b_ = b_ptr;
                auto i_ = i_ptr + x * ws1;
                for (auto w0 = weights_.begin(); w0 < weights_.end(); w0 += ws0)
                    *(t_ptr++) = std::inner_product(w0, w0 + ws0, i_, *(b_++));
            }
            return activation_(tmp);
        }
    }
}
conv2d.cc/      0           0     0     644     1685      `
﻿#include "conv2d.h"
namespace keras2cpp{
    namespace layers{
        Conv2D::Conv2D(Stream& file)
        : weights_(file, 4), biases_(file), activation_(file) {}

        Tensor Conv2D::operator()(const Tensor& in) const noexcept {
            kassert(in.dims_[2] == weights_.dims_[3]);

            auto& ww = weights_.dims_;

            size_t offset_y = ww[1] - 1;
            size_t offset_x = ww[2] - 1;
            auto tmp
                = Tensor::empty(in.dims_[0] - offset_y, in.dims_[1] - offset_x, ww[0]);

            auto ws_ = cast(ww[3] * ww[2] * ww[1] * ww[0]);
            auto ws0 = cast(ww[3] * ww[2] * ww[1]);
            auto ws1 = cast(ww[3] * ww[2]);
            auto ws2 = cast(ww[3]);
            auto is0 = cast(ww[3] * in.dims_[1]);

            auto ty = cast(tmp.dims_[0]);
            auto tx = cast(tmp.dims_[1]);

            auto w_ptr = weights_.begin();
            auto b_ptr = biases_.begin();
            auto t_ptr = std::back_inserter(tmp.data_);
            auto i_ptr = in.begin();

            for (ptrdiff_t y = 0; y < ty; ++y)
                for (ptrdiff_t x = 0; x < tx; ++x) {
                    auto b_ = b_ptr;
                    auto i_ = i_ptr + y * is0 + x * ws2;
                    for (auto w0 = w_ptr; w0 < w_ptr + ws_; w0 += ws0) {
                        auto tmp_ = 0.f;
                        auto i0 = i_;
                        for (auto w1 = w0; w1 < w0 + ws0; w1 += ws1, i0 += is0)
                            tmp_ = std::inner_product(w1, w1 + ws1, i0, tmp_);
                        *(++t_ptr) = *(b_++) + tmp_;
                    }
                }
            return activation_(tmp);
        }
    }
}
dense.cc/       0           0     0     644     894       `
﻿#include "dense.h"
namespace keras2cpp{
    namespace layers{
        Dense::Dense(Stream& file)
        : weights_(file, 2), biases_(file), activation_(file) {}

        Tensor Dense::operator()(const Tensor& in) const noexcept {
            kassert(in.dims_.back() == weights_.dims_[1]);
            const auto ws = cast(weights_.dims_[1]);

            Tensor tmp;
            tmp.dims_ = in.dims_;
            tmp.dims_.back() = weights_.dims_[0];
            tmp.data_.reserve(tmp.size());

            auto tmp_ = std::back_inserter(tmp.data_);
            for (auto in_ = in.begin(); in_ < in.end(); in_ += ws) {
                auto bias_ = biases_.begin();
                for (auto w = weights_.begin(); w < weights_.end(); w += ws)
                    *(tmp_++) = std::inner_product(w, w + ws, in_, *(bias_++));
            }
            return activation_(tmp);
        }
    }
}elu.cc/         0           0     0     644     558       `
﻿#include "elu.h"
namespace keras2cpp{
    namespace layers{
        ELU::ELU(Stream& file) : alpha_(file) {}    
        Tensor ELU::operator()(const Tensor& in) const noexcept {
            kassert(in.ndim());
            Tensor out;
            out.data_.resize(in.size());
            out.dims_ = in.dims_;

            std::transform(in.begin(), in.end(), out.begin(), [this](float x) {
                if (x >= 0.f)
                    return x;
                return alpha_ * std::expm1(x);
            });
            return out;
        }
    }
}embedding.cc/   0           0     0     644     646       `
﻿#include "embedding.h"
namespace keras2cpp{
    namespace layers{
        Embedding::Embedding(Stream& file) : weights_(file, 2) {}

        Tensor Embedding::operator()(const Tensor& in) const noexcept {
            size_t out_i = in.dims_[0];
            size_t out_j = weights_.dims_[1];

            auto out = Tensor::empty(out_i, out_j);

            for (const auto& it : in.data_) {
                auto first = weights_.begin() + cast(it * out_j);
                auto last = weights_.begin() + cast(it * out_j + out_j);
                out.data_.insert(out.end(), first, last);
            }
            return out;
        }
    }
}locally1d.cc/   0           0     0     644     1172      `
﻿#include "locally1d.h"
namespace keras2cpp{
    namespace layers{
        LocallyConnected1D::LocallyConnected1D(Stream& file)
        : weights_(file, 3), biases_(file, 2), activation_(file) {}

        Tensor LocallyConnected1D::operator()(const Tensor& in) const noexcept {
            auto& ww = weights_.dims_;

            size_t ksize = ww[2] / in.dims_[1];
            kassert(in.dims_[0] + 1 == ww[0] + ksize);

            auto tmp = Tensor::empty(ww[0], ww[1]);

            auto is0 = cast(in.dims_[1]);
            auto ts0 = cast(ww[1]);
            auto ws0 = cast(ww[2] * ww[1]);
            auto ws1 = cast(ww[2]);

            auto i_ptr = in.begin();
            auto b_ptr = biases_.begin();
            auto t_ptr = std::back_inserter(tmp.data_);

            for (auto w_ = weights_.begin(); w_ < weights_.end();
                 w_ += ws0, b_ptr += ts0, i_ptr += is0) {
                auto b_ = b_ptr;
                auto i_ = i_ptr;
                for (auto w0 = w_; w0 < w_ + ws0; w0 += ws1)
                    *(t_ptr++) = std::inner_product(w0, w0 + ws1, i_, *(b_++));
            }
            return activation_(tmp);
        }
    }
}
locally2d.cc/   0           0     0     644     1538      `
﻿#include "locally2d.h"
namespace keras2cpp{
    namespace layers{
        LocallyConnected2D::LocallyConnected2D(Stream& file)
        : weights_(file, 4), biases_(file, 3), activation_(file) {}

        Tensor LocallyConnected2D::operator()(const Tensor& in) const noexcept {
            /*
            // 'in' have shape (x, y, features)
            // 'tmp' have shape (new_x, new_y, outputs)
            // 'weights' have shape (new_x*new_y, outputs, kernel*features)
            // 'biases' have shape (new_x*new_y, outputs)
            auto& ww = weights_.dims_;

            size_t ksize = ww[2] / in.dims_[1];
            size_t offset = ksize - 1;
            kassert(in.dims_[0] - offset == ww[0]);

            auto tmp = Tensor::empty(ww[0], ww[1]);

            auto is0 = cast(in.dims_[1]);
            auto ts0 = cast(ww[1]);
            auto ws0 = cast(ww[2] * ww[1]);
            auto ws1 = cast(ww[2]);

            auto b_ptr = biases_.begin();
            auto t_ptr = tmp.begin();
            auto i_ptr = in.begin();

            for (auto w_ = weights_.begin(); w_ < weights_.end();
                 w_ += ws0, b_ptr += ts0, t_ptr += ts0, i_ptr += is0) {
                auto b_ = b_ptr;
                auto t_ = t_ptr;
                auto i_ = i_ptr;
                for (auto w0 = w_; w0 < w_ + ws0; w0 += ws1)
                    *(t_++) = std::inner_product(w0, w0 + ws1, i_, *(b_++));
            }
            return activation_(tmp);
            */
            return activation_(in);
        }
    }
}
lstm.cc/        0           0     0     644     2062      `
﻿#include "lstm.h"
#include <tuple>
namespace keras2cpp{
    namespace layers{
        LSTM::LSTM(Stream& file)
        : Wi_(file, 2)
        , Ui_(file, 2)
        , bi_(file, 2) // Input
        , Wf_(file, 2)
        , Uf_(file, 2)
        , bf_(file, 2) // Forget
        , Wc_(file, 2)
        , Uc_(file, 2)
        , bc_(file, 2) // State
        , Wo_(file, 2)
        , Uo_(file, 2)
        , bo_(file, 2) // Output
        , inner_activation_(file)
        , activation_(file)
        , return_sequences_(static_cast<unsigned>(file)) {}

        Tensor LSTM::operator()(const Tensor& in) const noexcept {
            // Assume 'bo_' always keeps the output shape and we will always
            // receive one single sample.
            size_t out_dim = bo_.dims_[1];
            size_t steps = in.dims_[0];

            Tensor c_tm1 {1, out_dim};

            if (!return_sequences_) {
                Tensor out {1, out_dim};
                for (size_t s = 0; s < steps; ++s)
                    std::tie(out, c_tm1) = step(in.select(s), out, c_tm1);
                return out.flatten();
            }

            auto out = Tensor::empty(steps, out_dim);
            Tensor last {1, out_dim};

            for (size_t s = 0; s < steps; ++s) {
                std::tie(last, c_tm1) = step(in.select(s), last, c_tm1);
                out.data_.insert(out.end(), last.begin(), last.end());
            }
            return out;
        }

        std::tuple<Tensor, Tensor>
        LSTM::step(const Tensor& x, const Tensor& h_tm1, const Tensor& c_tm1) const
            noexcept {
            auto i_ = x.dot(Wi_) + h_tm1.dot(Ui_) + bi_;
            auto f_ = x.dot(Wf_) + h_tm1.dot(Uf_) + bf_;
            auto c_ = x.dot(Wc_) + h_tm1.dot(Uc_) + bc_;
            auto o_ = x.dot(Wo_) + h_tm1.dot(Uo_) + bo_;

            auto cc = inner_activation_(f_) * c_tm1
                + inner_activation_(i_) * activation_(c_);
            auto out = inner_activation_(o_) * activation_(cc);
            return std::make_tuple(out, cc);
        }
    }
}maxPooling2d.cc/0           0     0     644     1404      `
﻿#include "maxPooling2d.h"
namespace keras2cpp{
    namespace layers{
        MaxPooling2D::MaxPooling2D(Stream& file)
        : pool_size_y_(file), pool_size_x_(file) {}

        Tensor MaxPooling2D::operator()(const Tensor& in) const noexcept {
            kassert(in.ndim() == 3);

            const auto& iw = in.dims_;

            Tensor out {iw[0] / pool_size_y_, iw[1] / pool_size_x_, iw[2]};
            out.fill(-std::numeric_limits<float>::infinity());

            auto is0p = cast(iw[2] * iw[1] * pool_size_y_);
            auto is0 = cast(iw[2] * iw[1]);
            auto is1p = cast(iw[2] * pool_size_x_);
            auto is1 = cast(iw[2]);
            auto os_ = cast(iw[2] * out.dims_[1] * out.dims_[0]);
            auto os0 = cast(iw[2] * out.dims_[1]);

            auto o_ptr = out.begin();
            auto i_ptr = in.begin();
            for (auto o0 = o_ptr; o0 < o_ptr + os_; o0 += os0, i_ptr += is0p) {
                auto i_ = i_ptr;
                for (auto o1 = o0; o1 < o0 + os0; o1 += is1, i_ += is1p)
                    for (auto i0 = i_; i0 < i_ + is0p; i0 += is0)
                        for (auto i1 = i0; i1 < i0 + is1p; i1 += is1)
                            std::transform(i1, i1 + is1, o1, o1, [](float x, float y) {
                                return std::max(x, y);
                            });
            }
            return out;
        }
    }
}